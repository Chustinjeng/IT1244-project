{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to our Machine Learning Project for IT1244\n",
    "\n",
    "We are group <group number> and we would like to investigate the effects of using bigrams and trigrams in the accuracy of predicting the sentiments of movie reviews. \n",
    "\n",
    "## Why bigrams and trigrams\n",
    "\n",
    "The concept of using N-grams is a fundamental practice in the realm of Natural Language Processing. N-grams refer to a contiguous sequences of n items in a body of text grouped together. More specifically, bigrams refer to pairs of consecutive words while trigrams refer to groups of 3 consecutive words. N-grams are used to capture context, allowing machines to accurately accomplish tasks such as generating texts and predicting sentiments.\n",
    "\n",
    "### Our approach\n",
    "\n",
    "We decided to use neural networks as our training model, rather than recurrent neural networks, because we feel that NNs can help with capturing long-term dependencies in sequences more than RNNs.\n",
    "\n",
    "First, let us import some modules to help us train our model. We set manual seed to be 0 (or could be any other value) to ensure that the dropout values are consistent in the later part of training the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step: Setting up the Dataset class\n",
    "\n",
    "Pytorch has a dataset class that allows us to use data in batches, rather than using the data wholesale. We want to implement a `SentDataset` class that inherits the property of the Dataset class in Pytorch.\n",
    "\n",
    "Some more information about the class methods:\n",
    "- `generate_bigrams` generates a list of bigrams from every movie review\n",
    "- `generate_trigrams` generates a list of trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A pytorch dataset class that accepts a text path, and optionally label path (only during training phase) and\n",
    "    a vocabulary (only during testing phase). This class holds all the data and implement\n",
    "    a __getitem__ method to be used by a Python generator object or other classes that need it.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path, type, label_path=None, vocab=None):\n",
    "        \"\"\"\n",
    "        Read the content of vocab and text_file\n",
    "        Args:\n",
    "            vocab (string): Path to the vocabulary file.\n",
    "            text_file (string): Path to the text file.\n",
    "            type (string): Specify if model is trained using bigrams or trigrams\n",
    "        \"\"\"\n",
    "        self.label_path = label_path\n",
    "        self.type = type\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        with open(train_path, encoding='utf-8') as f:\n",
    "            self.texts = [line for line in f.readlines() if line.strip()]\n",
    "        if label_path:\n",
    "            with open(label_path, encoding='utf-8') as f:\n",
    "                self.labels = [line for line in f.readlines() if line.strip()]\n",
    "        if not vocab:\n",
    "            self.vocabulary = {}\n",
    "            curr_idx = 0\n",
    "            for text in self.texts:\n",
    "                ngrams = self.generate_bigrams_or_trigrams(text, type)\n",
    "                for ngram in ngrams:\n",
    "                    if ngram in self.vocabulary:\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.vocabulary[ngram] = curr_idx\n",
    "                        curr_idx += 1\n",
    "        else: \n",
    "            self.vocabulary = vocab\n",
    "        \n",
    "\n",
    "    def generate_bigrams(self, text):\n",
    "        \"\"\"\n",
    "        Function to generate bigrams from a text (string)\n",
    "        Bigrams are defined as a grouping of a text into a list of 2 consecutive words\n",
    "        \"\"\"\n",
    "        tokens = text.split()\n",
    "        bigrams = []\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = f\"{tokens[i]} {tokens[i + 1]}\"\n",
    "            bigrams.append(bigram)\n",
    "        return bigrams\n",
    "    \n",
    "    def generate_trigrams(self, text):\n",
    "        \"\"\"\n",
    "        Function to generate bigrams from a text (string)\n",
    "        Bigrams are defined as a grouping of a text into a list of 3 consecutive words\n",
    "        \"\"\"\n",
    "        tokens = text.split()\n",
    "        trigrams = []\n",
    "        for i in range(len(tokens) - 2):\n",
    "            trigram = f\"{tokens[i]} {tokens[i + 1]} {tokens[i + 2]}\"\n",
    "            trigrams.append(trigram)\n",
    "        return trigrams\n",
    "    \n",
    "    def generate_bigrams_or_trigrams(self, text, type):\n",
    "        \"\"\"\n",
    "        Function to determine if bigrams or trigrams should be generated, depending on type specified\n",
    "        \"\"\"\n",
    "        if type == \"bigram\":\n",
    "            return self.generate_bigrams(text)\n",
    "        else: \n",
    "            return self.generate_trigrams(text)\n",
    "\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "        A function to inform the vocab size. The function returns two numbers:\n",
    "            num_vocab: size of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of instances in the data\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Return the i-th instance in the format of:\n",
    "            (text, label)\n",
    "        Text and label is encoded according to the vocab (word_id).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.label_path: # training\n",
    "            text = self.texts[i]\n",
    "            label = int(self.labels[i])\n",
    "            indices = []\n",
    "            ngrams_in_text = self.generate_bigrams_or_trigrams(text, self.type)\n",
    "            for ngram in ngrams_in_text:\n",
    "                index = self.vocabulary.get(ngram)\n",
    "                indices.append(index)\n",
    "    \n",
    "            indices_tensor = torch.tensor(indices)\n",
    "            return indices_tensor, label\n",
    "        \n",
    "        else: # testing \n",
    "            text = self.texts[i]\n",
    "            indices = []\n",
    "            ngrams_in_text = self.generate_bigrams_or_trigrams(text, self.type)\n",
    "            for ngram in ngrams_in_text:\n",
    "                if ngram in self.vocabulary:\n",
    "                    index = self.vocabulary.get(ngram)\n",
    "                    indices.append(index)\n",
    "            indices_tensor = torch.tensor(indices)\n",
    "            return indices_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step: Setting up the model\n",
    "\n",
    "As mentioned earlier, we decided to use neural networks as our model framework. We have also came up with custom layers to pass our data through.\n",
    "\n",
    "*TODO: explain more on embedding*\n",
    "\n",
    "These layers are (in order):\n",
    "- an embedding layer that takes in the number of vocabulary to place a limit on the number of embeddings needed, as well as the embedding dimension, which is a hyperparameter.\n",
    "- a first linear layer with output dimension, which is a parameter.\n",
    "- a RELU activation function after the first linear layer\n",
    "- a dropout layer with hyperparameter probability to reduce overfitting\n",
    "- a second linear layer with output dimension 1\n",
    "- a final sigmoid layer to output values 0 to 1, representing the probability of the movie review to be positive (1) or negative (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, num_vocab):\n",
    "        super().__init__()\n",
    "        # define model attributes \n",
    "        self.embedding_dim = 8 # define embedding dimensions (hyperparameter)\n",
    "        self.embedding = nn.Embedding(num_vocab, self.embedding_dim) # transform words into embeddings\n",
    "        self.first_layer_dim = 24 # define first layer dimension (hyperparameter)\n",
    "        self.linear_layer_1 = nn.Linear(self.embedding_dim, self.first_layer_dim) # linear layer\n",
    "        self.relu = nn.ReLU() # ReLU activation function\n",
    "        self.dropout = nn.Dropout(0.2) # dropout of 0.2 probability (hyperparameter) to reduce overfitting\n",
    "        self.linear_layer_2 = nn.Linear(self.first_layer_dim, 1) # last linear layer\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid function to determine probabilities\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.linear_layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_layer_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third step: Define a collator function\n",
    "\n",
    "A collator function is used during the loading of batches of data during testing. This function returns a pair of tensors that represent the texts in the batch, as well as the labels of the texts.\n",
    "\n",
    "Note that the dimensions of the text may differ from each other, so we padded the tensor with zeros to ensure that we can fit multiple texts of different sizes into one big tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    \"\"\"\n",
    "    A function that receives a list of (text, label) pair\n",
    "    and return a pair of tensors:\n",
    "        texts: a tensor that combines all the text in the mini-batch, pad with 0\n",
    "        labels: a tensor that combines all the labels in the mini-batch\n",
    "    \"\"\"\n",
    "    if len(batch[0]) == 2:\n",
    "        texts, labels = zip(*batch)\n",
    "        # convert text indices to tensor\n",
    "        texts_tensor = nn.utils.rnn.pad_sequence([text for text in texts], batch_first=True, padding_value=0)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "        return texts_tensor, labels_tensor\n",
    "    else:\n",
    "        texts_tensor = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "        return texts_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth step: Define the training function\n",
    "\n",
    "Next, we would have to train the model. We used the `DataLoader` class provided by pytorch to load data in batches, with the help of the `collator` function we just initialised.\n",
    "\n",
    "The loss function we have chosen is the Binary Cross Entropy Loss, as the labels are in binary form (0 or 1). The optimiser we chose is Adam's optimiser.\n",
    "\n",
    "In each epoch, we will train the model using batches of data. We would then do forward propagation of the data, calculate the loss, and finally update the weights of the model.\n",
    "\n",
    "After training, we would save the state of the model and the optimizer into a checkpoint file. This file is to be used later during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size, learning_rate, num_epoch, device='cpu', model_path=None):\n",
    "    \"\"\"\n",
    "    Complete the training procedure below by specifying the loss function\n",
    "    and optimizers with the specified learning rate and specified number of epoch.\n",
    "    \n",
    "    \"\"\"\n",
    "    # instantiate the data loader which loads data in batches\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator, shuffle=True)\n",
    "\n",
    "    # loss function is Binary Cross Entropy Loss\n",
    "    criterion = nn.BCELoss()\n",
    "    # optimiser is Adam's optimiser\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for step, data in enumerate(data_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            texts = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # do forward propagation\n",
    "            outputs = model(texts)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels.reshape((outputs.shape[0], 1)))\n",
    "\n",
    "            # do backward propagation to update the weights\n",
    "            loss.backward()\n",
    "\n",
    "            # do the parameter optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate running loss value for non padding\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print loss value every 100 iterations and reset running loss\n",
    "            if step % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, step + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # make the checkpoint of the model and save it to the model path\n",
    "    # contains current state of the model, optimiser, number of epochs, and current vocabulary\n",
    "    checkpoint = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'epoch': num_epoch,\n",
    "        'vocab': dataset.vocabulary\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "    print('Model saved in ', model_path)\n",
    "    print('Training finished in {} minutes.'.format((end - start).seconds / 60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth step: define the testing function\n",
    "\n",
    "With the same logic as the training function, we load batches from the test dataset to predict their labels. Using 0.5 as the threshold, results that are 0.5 and above would be a positive sentiment, while those below 0.5 would be a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, thres=0.5, device='cpu'):\n",
    "    model.eval()\n",
    "    data_loader = DataLoader(dataset, batch_size=20, collate_fn=collator, shuffle=False)\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            texts = data.to(device)\n",
    "            results = model(texts)\n",
    "            pred_labels = (results > thres).int().tolist()\n",
    "            pred_labels = sum(pred_labels, [])\n",
    "            labels.extend(pred_labels)\n",
    "\n",
    "    return [str(x) for x in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almost there\n",
    "\n",
    "Let's initialise variables useful to us such as the path to training dataset, the testing dataset, and the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = \"x_train.txt\"\n",
    "x_test = \"x_test.txt\"\n",
    "y_train = \"y_train.txt\"\n",
    "y_test = \"y_test.txt\"\n",
    "output_path = \"out.txt\"\n",
    "model_checkpoint = \"model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally: train the model!\n",
    "\n",
    "We have now initialised all the functions that we needed. Now it is time to run the code. \n",
    "\n",
    "During the training phase, we need to initialise the SentDataset model with the path of the training dataset and the labels. Then, we pass the number of vocab as a parameter into the model. We specify hyper-parameters of such as batch size, learning rate, and number of epoch. \n",
    "\n",
    "The function below trains the model using bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.695\n",
      "[1,   200] loss: 0.691\n",
      "[1,   300] loss: 0.660\n",
      "[1,   400] loss: 0.496\n",
      "[1,   500] loss: 0.382\n",
      "[2,   100] loss: 0.166\n",
      "[2,   200] loss: 0.147\n",
      "[2,   300] loss: 0.136\n",
      "[2,   400] loss: 0.138\n",
      "[2,   500] loss: 0.119\n",
      "[3,   100] loss: 0.027\n",
      "[3,   200] loss: 0.023\n",
      "[3,   300] loss: 0.027\n",
      "[3,   400] loss: 0.042\n",
      "[3,   500] loss: 0.020\n",
      "[4,   100] loss: 0.010\n",
      "[4,   200] loss: 0.012\n",
      "[4,   300] loss: 0.008\n",
      "[4,   400] loss: 0.029\n",
      "[4,   500] loss: 0.009\n",
      "[5,   100] loss: 0.004\n",
      "[5,   200] loss: 0.005\n",
      "[5,   300] loss: 0.005\n",
      "[5,   400] loss: 0.005\n",
      "[5,   500] loss: 0.005\n",
      "[6,   100] loss: 0.002\n",
      "[6,   200] loss: 0.005\n",
      "[6,   300] loss: 0.003\n",
      "[6,   400] loss: 0.003\n",
      "[6,   500] loss: 0.002\n",
      "[7,   100] loss: 0.004\n",
      "[7,   200] loss: 0.004\n",
      "[7,   300] loss: 0.004\n",
      "[7,   400] loss: 0.001\n",
      "[7,   500] loss: 0.001\n",
      "[8,   100] loss: 0.090\n",
      "[8,   200] loss: 0.008\n",
      "[8,   300] loss: 0.006\n",
      "[8,   400] loss: 0.003\n",
      "[8,   500] loss: 0.002\n",
      "[9,   100] loss: 0.003\n",
      "[9,   200] loss: 0.002\n",
      "[9,   300] loss: 0.001\n",
      "[9,   400] loss: 0.003\n",
      "[9,   500] loss: 0.002\n",
      "[10,   100] loss: 0.002\n",
      "[10,   200] loss: 0.002\n",
      "[10,   300] loss: 0.001\n",
      "[10,   400] loss: 0.001\n",
      "[10,   500] loss: 0.001\n",
      "Model saved in  model.pt\n",
      "Training finished in 3.3333333333333335 minutes.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_str = 'cuda:{}'.format(0)\n",
    "else:\n",
    "    device_str = 'cpu'\n",
    "    device = torch.device(device_str)\n",
    "\n",
    "dataset = SentDataset(x_train, \"bigram\", y_train)\n",
    "num_vocab = dataset.vocab_size()\n",
    "model = Model(num_vocab).to(device)\n",
    "\n",
    "# specify hyper-parameters\n",
    "batch_size = 48\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "train(model, dataset, batch_size, learning_rate, num_epochs, device, model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code excerpt below to train the model using trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_str = 'cuda:{}'.format(0)\n",
    "else:\n",
    "    device_str = 'cpu'\n",
    "    device = torch.device(device_str)\n",
    "\n",
    "dataset = SentDataset(x_train, \"trigram\", y_train)\n",
    "num_vocab = dataset.vocab_size()\n",
    "model = Model(num_vocab).to(device)\n",
    "\n",
    "# specify hyper-parameters\n",
    "batch_size = 48\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "train(model, dataset, batch_size, learning_rate, num_epochs, device, model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Now that you have finished training the model, it is time to test the model. \n",
    "\n",
    "We first load the model checkpoint saved during the testing phase. Similarly to our training phase, we pass our testing dataset into our SentDataset class. Then, we load the model using the checkpoint saved.\n",
    "\n",
    "Following which, we test the model using the testing dataset, outputting the predicted labels into the output path as specified earlier.\n",
    "\n",
    "Run the code excerpt below if you have trained your model using `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the checkpoint\n",
    "checkpoint = torch.load(model_checkpoint)\n",
    "\n",
    "# create the test dataset object using SentDataset class\n",
    "dataset = SentDataset(x_test, \"bigram\" , vocab=checkpoint[\"vocab\"])\n",
    "\n",
    "# initialize and load the model\n",
    "num_vocab = dataset.vocab_size()\n",
    "model = Model(num_vocab).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "\n",
    "# run the prediction\n",
    "preds = test(model, dataset, 0.5, device)\n",
    "# write the output\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code excerpt below if you have trained your model using `trigrams`\n",
    "\n",
    "**Note**: you should run the testing model that corresponds to your method of training. For example, if you trained your model using bigrams, you should test your model using bigrams. This is to ensure maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the checkpoint\n",
    "checkpoint = torch.load(model_checkpoint)\n",
    "\n",
    "# create the test dataset object using SentDataset class\n",
    "dataset = SentDataset(x_test, \"bigram\" , vocab=checkpoint[\"vocab\"])\n",
    "\n",
    "# initialize and load the model\n",
    "num_vocab = dataset.vocab_size()\n",
    "model = Model(num_vocab).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "\n",
    "# run the prediction\n",
    "preds = test(model, dataset, 0.5, device)\n",
    "# write the output\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your results!\n",
    "\n",
    "Finally, it is time to evaluate the accuracy of the model. To do this, we compare the predicted labels against the actual test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.91%\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, encoding='utf-8') as f:\n",
    "    preds = [l.strip() for l in f.readlines()]\n",
    "with open(y_test, encoding='utf-8') as f:\n",
    "    labels = [l.strip() for l in f.readlines()]\n",
    "assert len(preds) == len(labels), \"Length of predictions ({}) and labels ({}) are not the same\"\\\n",
    "    .format(len(preds), len(labels))\n",
    "\n",
    "correct = 0\n",
    "for pred, label in zip(preds, labels):\n",
    "    if pred == label:\n",
    "        correct += 1\n",
    "print('Accuracy: {:.2f}%'.format((100.0 * correct) / len(labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
